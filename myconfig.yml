model_list:
  - model_name: gpt-3.5-turbo
    litellm_params: 
      model: vertex_ai/gemini-1.5-flash-preview-0514
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-pro
    litellm_params: 
      model: vertex_ai/gemini-1.5-flash-preview-0514
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-pro-vision
    litellm_params: 
      model: vertex_ai/gemini-pro-vision
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-1.5-pro
    litellm_params: 
      model: vertex_ai/gemini-1.5-pro-preview-0409
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-1.5-pro-latest
    litellm_params: 
      model: vertex_ai/gemini-1.5-pro-preview-0409
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-1.5-pro-json
    litellm_params: 
      model: vertex_ai/gemini-1.5-pro-preview-0409
      response_mime_type: application/json
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-1.5-flash
    litellm_params: 
      model: vertex_ai/gemini-1.5-flash-preview-0514
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  - model_name: gemini-1.5-flash-latest
    litellm_params: 
      model: vertex_ai/gemini-1.5-flash-preview-0514
      safety_settings:
      - category: HARM_CATEGORY_HARASSMENT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_HATE_SPEECH
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
        threshold: BLOCK_ONLY_HIGH
      - category: HARM_CATEGORY_DANGEROUS_CONTENT
        threshold: BLOCK_ONLY_HIGH
  # - model_name: gpt-3.5-turbo
  #   litellm_params:
  #     model: azure/chatgpt-v-2
  #     api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
  #     api_version: "2023-05-15"
  #     api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  # - model_name: gpt-3.5-turbo-large
  #   litellm_params: 
  #     model: "gpt-3.5-turbo-1106"
  #     api_key: os.environ/OPENAI_API_KEY
  # - model_name: gpt-4
  #   litellm_params:
  #     model: azure/chatgpt-v-2
  #     api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
  #     api_version: "2023-05-15"
  #     api_key: os.environ/AZURE_API_KEY # The `os.environ/` prefix tells litellm to read this from the env. See https://docs.litellm.ai/docs/simple_proxy#load-api-keys-from-vault
  # - model_name: sagemaker-completion-model
  #   litellm_params:
  #     model: sagemaker/berri-benchmarking-Llama-2-70b-chat-hf-4
  #     input_cost_per_second: 0.000420  
  # - model_name: text-embedding-ada-002
  #   litellm_params: 
  #     model: azure/azure-embedding-model
  #     api_key: os.environ/AZURE_API_KEY
  #     api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
  #     api_version: "2023-05-15"
  #   model_info:
  #     mode: embedding
  #     base_model: text-embedding-ada-002
  # - model_name: dall-e-2
  #   litellm_params:
  #     model: azure/
  #     api_version: 2023-06-01-preview
  #     api_base: https://openai-gpt-4-test-v-1.openai.azure.com/
  #     api_key: os.environ/AZURE_API_KEY
  # - model_name: openai-dall-e-3
  #   litellm_params:
  #     model: dall-e-3
  # - model_name: fake-openai-endpoint
  #   litellm_params:
  #     model: openai/fake
  #     api_key: fake-key
  #     api_base: https://exampleopenaiendpoint-production.up.railway.app/

litellm_settings:
  #vertex_project: "" # Your Project ID
  vertex_location: "asia-northeast1" # proj location
  drop_params: True
  add_function_to_prompt: True
  max_budget: 100 
  budget_duration: 30d
  num_retries: 1
  request_timeout: 300
  telemetry: False
  #context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]
  cache: True          # set cache responses to True, litellm defaults to using a redis cache
  cache_params:         # cache_params are optional
    type: local         # Type of cache (options: "local", "redis", "s3")

general_settings: 
  master_key: os.environ/MC_KEY # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

# environment_variables:
  # settings for using redis caching
  # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
  # REDIS_PORT: "16337"
  # REDIS_PASSWORD: 
